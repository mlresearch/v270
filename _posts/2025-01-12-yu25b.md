---
title: Learning Visual Parkour from Generated Images
abstract: Fast and accurate physics simulation is an essential component of robot
  learning, where robots can explore failure scenarios that are difficult to produce
  in the real world and learn from unlimited on-policy data. Yet, it remains challenging
  to incorporate RGB-color perception into the sim-to-real pipeline that matches the
  real world in its richness and realism. In this work, we train a robot dog in simulation
  for visual parkour. We propose a way to use generative models to synthesize diverse
  and physically accurate image sequences of the scene from the robotâ€™s ego-centric
  perspective. We present demonstrations of zero-shot transfer to the RGB-only observations
  of the real world on a robot equipped with a low-cost, off-the-shelf color camera.
section: Poster
openreview: cGswIOxHcN
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu25b
month: 0
tex_title: Learning Visual Parkour from Generated Images
firstpage: 2500
lastpage: 2516
page: 2500-2516
order: 2500
cycles: false
bibtex_author: Yu, Alan and Yang, Ge and Choi, Ran and Ravan, Yajvan and Leonard,
  John and Isola, Phillip
author:
- given: Alan
  family: Yu
- given: Ge
  family: Yang
- given: Ran
  family: Choi
- given: Yajvan
  family: Ravan
- given: John
  family: Leonard
- given: Phillip
  family: Isola
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/yu25b/yu25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
