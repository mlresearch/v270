---
title: Scaling Manipulation Learning with Visual Kinematic Chain Prediction
abstract: Learning general-purpose models from diverse datasets has achieved great
  success in machine learning. In robotics, however, existing methods in multi-task
  learning are typically constrained to a single robot and workspace, while recent
  work such as RT-X requires a non-trivial action normalization procedure to manually
  bridge the gap between different action spaces in diverse environments. In this
  paper, we propose the visual kinematics chain as a precise and universal representation
  of quasi-static actions for robot learning over diverse environments, which requires
  no manual adjustment since the visual kinematic chains can be automatically obtained
  from the robotâ€™s model and camera parameters. We propose the Visual Kinematics Transformer
  (VKT), a convolution-free architecture that supports an arbitrary number of camera
  viewpoints, and that is trained with a single objective of forecasting kinematic
  structures through optimal point-set matching. We demonstrate the superior performance
  of VKT over BC transformers as a general agent on Calvin, RLBench, ALOHA, Open-X,
  and real robot manipulation tasks. Video demonstrations and source code can be found
  at https://mlzxy.github.io/visual-kinetic-chain.
section: Poster
openreview: Yw5QGNBkEN
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang25f
month: 0
tex_title: Scaling Manipulation Learning with Visual Kinematic Chain Prediction
firstpage: 2714
lastpage: 2728
page: 2714-2728
order: 2714
cycles: false
bibtex_author: Zhang, Xinyu and Liu, Yuhan and Chang, Haonan and Boularias, Abdeslam
author:
- given: Xinyu
  family: Zhang
- given: Yuhan
  family: Liu
- given: Haonan
  family: Chang
- given: Abdeslam
  family: Boularias
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/zhang25f/zhang25f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
