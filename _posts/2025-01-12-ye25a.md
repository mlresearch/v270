---
title: 'Reinforcement Learning with Foundation Priors: Let Embodied Agent Efficiently
  Learn on Its Own'
abstract: 'Reinforcement learning (RL) is a promising approach for solving robotic
  manipulation tasks. However, it is challenging to apply the RL algorithms directly
  in the real world. For one thing, RL is data-intensive and typically requires millions
  of interactions with environments, which are impractical in real scenarios. For
  another, it is necessary to make heavy engineering efforts to design reward functions
  manually. To address these issues, we leverage foundation models in this paper.
  We propose Reinforcement Learning with Foundation Priors (RLFP) to utilize guidance
  and feedback from policy, value, and success-reward foundation models. Within this
  framework, we introduce the Foundation-guided Actor-Critic (FAC) algorithm, which
  enables embodied agents to explore more efficiently with automatic reward functions.
  The benefits of our framework are threefold: (1) \textit{sample efficient}; (2)
  \textit{minimal and effective reward engineering}; (3) \textit{agnostic to foundation
  model forms and robust to noisy priors}. Our method achieves remarkable performances
  in various manipulation tasks on both real robots and in simulation. Across 5 dexterous
  tasks with real robots, FAC achieves an average success rate of 86% after one hour
  of real-time learning. Across 8 tasks in the simulated Meta-world, FAC achieves
  100% success rates in 7/8 tasks under less than 100k frames (about 1-hour training),
  outperforming baseline methods with manual-designed rewards in 1M frames. We believe
  the RLFP framework can enable future robots to explore and learn autonomously in
  the physical world for more tasks.'
section: Oral
openreview: dsxmR6lYlg
software: https://github.com/YeWR/RLFP
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ye25a
month: 0
tex_title: 'Reinforcement Learning with Foundation Priors: Let Embodied Agent Efficiently
  Learn on Its Own'
firstpage: 185
lastpage: 208
page: 185-208
order: 185
cycles: false
bibtex_author: Ye, Weirui and Zhang, Yunsheng and Weng, Haoyang and Gu, Xianfan and
  Wang, Shengjie and Zhang, Tong and Wang, Mengchen and Abbeel, Pieter and Gao, Yang
author:
- given: Weirui
  family: Ye
- given: Yunsheng
  family: Zhang
- given: Haoyang
  family: Weng
- given: Xianfan
  family: Gu
- given: Shengjie
  family: Wang
- given: Tong
  family: Zhang
- given: Mengchen
  family: Wang
- given: Pieter
  family: Abbeel
- given: Yang
  family: Gao
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/ye25a/ye25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
