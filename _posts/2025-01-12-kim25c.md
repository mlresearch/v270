---
title: 'OpenVLA: An Open-Source Vision-Language-Action Model'
abstract: 'Large policies pretrained on a combination of Internet-scale vision-language
  data and diverse robot demonstrations have the potential to change how we teach
  robots new skills: rather than training new behaviors from scratch, we can fine-tune
  such vision-language-action (VLA) models to obtain robust, generalizable policies
  for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging
  as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior
  work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a
  key component for adoption. Addressing these challenges, we introduce OpenVLA, a
  7B-parameter open-source VLA trained on a diverse collection of 970k real-world
  robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a
  visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product
  of the added data diversity and new model components, OpenVLA demonstrates strong
  results for generalist manipulation, outperforming closed models such as RT-2-X
  (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot
  embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune
  OpenVLA for new settings, with especially strong generalization results in multi-task
  environments involving multiple objects and strong language grounding abilities,
  where we outperform expressive from-scratch imitation learning methods such as Diffusion
  Policy by 20.4%. We also explore compute efficiency; as a separate contribution,
  we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation
  methods and served efficiently via quantization without a hit to downstream success
  rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch
  codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.'
section: Poster
openreview: ZMnD6QZAE6
software: https://github.com/openvla/openvla
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim25c
month: 0
tex_title: 'OpenVLA: An Open-Source Vision-Language-Action Model'
firstpage: 2679
lastpage: 2713
page: 2679-2713
order: 2679
cycles: false
bibtex_author: Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao,
  Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan
  P and Sanketi, Pannag R and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin
  and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn,
  Chelsea
author:
- given: Moo Jin
  family: Kim
- given: Karl
  family: Pertsch
- given: Siddharth
  family: Karamcheti
- given: Ted
  family: Xiao
- given: Ashwin
  family: Balakrishna
- given: Suraj
  family: Nair
- given: Rafael
  family: Rafailov
- given: Ethan P
  family: Foster
- given: Pannag R
  family: Sanketi
- given: Quan
  family: Vuong
- given: Thomas
  family: Kollar
- given: Benjamin
  family: Burchfiel
- given: Russ
  family: Tedrake
- given: Dorsa
  family: Sadigh
- given: Sergey
  family: Levine
- given: Percy
  family: Liang
- given: Chelsea
  family: Finn
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/kim25c/kim25c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
