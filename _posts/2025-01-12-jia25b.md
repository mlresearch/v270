---
title: 'MaIL: Improving Imitation Learning with Selective State Space Models'
abstract: 'This work introduces Mamba Imitation Learning (MaIL), a novel imitation
  learning (IL) architecture that offers a computationally efficient alternative to
  state-of-the-art (SoTA) Transformer policies. Transformer-based policies have achieved
  remarkable results due to their ability in handling human-recorded data with inherently
  non-Markovian behavior. However, their high performance comes with the drawback
  of large models that complicate effective training. While state space models (SSMs)
  have been known for their efficiency, they were not able to match the performance
  of Transformers. Mamba significantly improves the performance of SSMs and rivals
  against Transformers, positioning it as an appealing alternative for IL policies.
  MaIL leverages Mamba as a backbone and introduces a formalism that allows using
  Mamba in the encoder-decoder structure. This formalism makes it a versatile architecture
  that can be used as a standalone policy or as part of a more advanced architecture,
  such as a diffuser in the diffusion process. Extensive evaluations on the LIBERO
  IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers
  in all LIBERO tasks, ii) achieves good performance even with small datasets, iii)
  is able to effectively process multi-modal sensory inputs, iv) is more robust to
  input noise compared to Transformers.'
section: Poster
openreview: IssXUYvVTg
software: https://github.com/ALRhub/MaIL
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jia25b
month: 0
tex_title: 'MaIL: Improving Imitation Learning with Selective State Space Models'
firstpage: 3888
lastpage: 3907
page: 3888-3907
order: 3888
cycles: false
bibtex_author: Jia, Xiaogang and Wang, Qian and Donat, Atalay and Xing, Bowen and
  Li, Ge and Zhou, Hongyi and Celik, Onur and Blessing, Denis and Lioutikov, Rudolf
  and Neumann, Gerhard
author:
- given: Xiaogang
  family: Jia
- given: Qian
  family: Wang
- given: Atalay
  family: Donat
- given: Bowen
  family: Xing
- given: Ge
  family: Li
- given: Hongyi
  family: Zhou
- given: Onur
  family: Celik
- given: Denis
  family: Blessing
- given: Rudolf
  family: Lioutikov
- given: Gerhard
  family: Neumann
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/jia25b/jia25b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
