---
title: Towards Open-World Grasping with Large Vision-Language Models
abstract: 'The ability to grasp objects in-the-wild from open-ended language instructions
  constitutes a fundamental challenge in robotics. An open-world grasping system should
  be able to combine high-level contextual with low-level physical-geometric reasoning
  in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale
  knowledge inherent in large language models (LLMs) to plan and reason in robotic
  context, but rely on external vision and action models to ground such knowledge
  into the environment and parameterize actuation. This setup suffers from two major
  bottlenecks: a) the LLM’s reasoning capacity is constrained by the quality of visual
  grounding, and b) LLMs do not contain low-level spatial understanding of the world,
  which is essential for grasping in contact-rich scenarios. In this work we demonstrate
  that modern vision-language models (VLMs) are capable of tackling such limitations,
  as they are implicitly grounded and can jointly reason about semantics and geometry.
  We propose \texttt{OWG}, an open-world grasping pipeline that combines VLMs with
  segmentation and grasp synthesis models to unlock grounded world understanding in
  three stages: open-ended referring segmentation, grounded grasp planning and grasp
  ranking via contact reasoning, all of which can be applied zero-shot via suitable
  visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor
  scene datasets to showcase \texttt{OWG}’s robustness in grounding from open-ended
  language, as well as open-world robotic grasping experiments in both simulation
  and hardware that demonstrate superior performance compared to previous supervised
  and zero-shot LLM-based methods.'
section: Poster
openreview: QUzwHYJ9Hf
software: https://github.com/gtziafas/OWG
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tziafas25a
month: 0
tex_title: Towards Open-World Grasping with Large Vision-Language Models
firstpage: 3304
lastpage: 3332
page: 3304-3332
order: 3304
cycles: false
bibtex_author: Tziafas, Georgios and Kasaei, Hamidreza
author:
- given: Georgios
  family: Tziafas
- given: Hamidreza
  family: Kasaei
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/tziafas25a/tziafas25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
