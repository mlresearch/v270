---
title: Differentiable Robot Rendering
abstract: Vision foundation models trained on massive amounts of visual data have
  shown unprecedented reasoning and planning skills in open-world settings. A key
  challenge in applying them to robotic tasks is the modality gap between visual data
  and action data. We introduce differentiable robot rendering, a method allowing
  the visual appearance of a robot body to be directly differentiable with respect
  to its control parameters. Our model integrates a kinematics-aware deformable model
  and Gaussians Splatting and is compatible with any robot form factors and degrees
  of freedom. We demonstrate its capability and usage in applications including reconstruction
  of robot poses from images and controlling robots through vision language models.
  Quantitative and qualitative results show that our differentiable rendering model
  provides effective gradients for robotic control directly from pixels, setting the
  foundation for the future applications of vision foundation models in robotics.
section: Oral
openreview: lt0Yf8Wh5O
software: https://github.com/cvlab-columbia/drrobot
video: https://drrobot.cs.columbia.edu/assets/videos/video.mp4
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu25a
month: 0
tex_title: Differentiable Robot Rendering
firstpage: 117
lastpage: 129
page: 117-129
order: 117
cycles: false
bibtex_author: Liu, Ruoshi and Canberk, Alper and Song, Shuran and Vondrick, Carl
author:
- given: Ruoshi
  family: Liu
- given: Alper
  family: Canberk
- given: Shuran
  family: Song
- given: Carl
  family: Vondrick
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/liu25a/liu25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
