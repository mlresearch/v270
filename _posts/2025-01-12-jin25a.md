---
title: Reasoning Grasping via Multimodal Large Language Model
abstract: 'Despite significant progress in robotic systems for operation within human-centric
  environments, existing models still heavily rely on explicit human commands to identify
  and manipulate specific objects. This limits their effectiveness in environments
  where understanding and acting on implicit human intentions are crucial. In this
  study, we introduce a novel task: reasoning grasping, where robots need to generate
  grasp poses based on indirect verbal instructions or intentions. To accomplish this,
  we propose an end-to-end reasoning grasping model that integrates a multimodal Large
  Language Model (LLM) with a vision-based robotic grasping framework. In addition,
  we present the first reasoning grasping benchmark dataset generated from the GraspNet-1
  billion, incorporating implicit instructions for object-level and part-level grasping,
  and this dataset will soon be available for public access. Our results show that
  directly integrating CLIP or LLaVA with the grasp detection model performs poorly
  on the challenging reasoning grasping tasks, while our proposed model demonstrates
  significantly enhanced performance both in the reasoning grasping benchmark and
  real-world experiments.'
section: Poster
openreview: KPcX4jetMw
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jin25a
month: 0
tex_title: Reasoning Grasping via Multimodal Large Language Model
firstpage: 3809
lastpage: 3827
page: 3809-3827
order: 3809
cycles: false
bibtex_author: Jin, Shiyu and XU, JINXUAN and Lei, Yutian and Zhang, Liangjun
author:
- given: Shiyu
  family: Jin
- given: JINXUAN
  family: XU
- given: Yutian
  family: Lei
- given: Liangjun
  family: Zhang
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/jin25a/jin25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
