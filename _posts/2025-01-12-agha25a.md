---
title: Exploring Under Constraints with Model-Based Actor-Critic and Safety Filters
abstract: Applying reinforcement learning (RL) to learn effective policies on physical
  robots without supervision remains challenging when it comes to tasks where safe
  exploration is critical. Constrained model-based RL (CMBRL) presents a promising
  approach to this problem. These methods are designed to learn constraint-adhering
  policies through constrained optimization approaches. Yet, such policies often fail
  to meet stringent safety requirements during learning and exploration. Our solution
  “CASE” aims to reduce the instances where constraints are breached during the learning
  phase. Specifically, CASE integrates techniques for optimizing constrained policies
  and employs planning-based safety filters as backup policies, effectively lowering
  constraint violations during learning and making it a more reliable option than
  other recent constrained model-based policy optimization methods.
section: Poster
openreview: s31IWg2kN5
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agha25a
month: 0
tex_title: Exploring Under Constraints with Model-Based Actor-Critic and Safety Filters
firstpage: 1216
lastpage: 1230
page: 1216-1230
order: 1216
cycles: false
bibtex_author: Agha, Ahmed and Kayalibay, Baris and Mirchev, Atanas and Smagt, Patrick
  van der and Bayer, Justin
author:
- given: Ahmed
  family: Agha
- given: Baris
  family: Kayalibay
- given: Atanas
  family: Mirchev
- given: Patrick van der
  family: Smagt
- given: Justin
  family: Bayer
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/agha25a/agha25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
