---
title: Solving Offline Reinforcement Learning with Decision Tree Regression
abstract: 'This study presents a novel approach to addressing offline reinforcement
  learning (RL) problems by reframing them as regression tasks that can be effectively
  solved using Decision Trees. Mainly, we introduce two distinct frameworks: return-conditioned
  and return-weighted decision tree policies (RCDTP and RWDTP), both of which achieve
  notable speed in agent training as well as inference, with training typically lasting
  less than a few minutes. Despite the simplification inherent in this reformulated
  approach to offline RL, our agents demonstrate performance that is at least on par
  with the established methods. We evaluate our methods on D4RL datasets for locomotion
  and manipulation, as well as other robotic tasks involving wheeled and flying robots.
  Additionally, we assess performance in delayed/sparse reward scenarios and highlight
  the explainability of these policies through action distribution and feature importance.'
section: Poster
openreview: eTRncsYYdv
software: https://github.com/PrajwalKoirala/Offline-Reinforcement-Learning-with-Decision-Tree-Regression/tree/main
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: koirala25a
month: 0
tex_title: Solving Offline Reinforcement Learning with Decision Tree Regression
firstpage: 2147
lastpage: 2163
page: 2147-2163
order: 2147
cycles: false
bibtex_author: Koirala, Prajwal and Fleming, Cody
author:
- given: Prajwal
  family: Koirala
- given: Cody
  family: Fleming
date: 2025-01-12
address:
container-title: Proceedings of The 8th Conference on Robot Learning
volume: '270'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 1
  - 12
pdf: https://raw.githubusercontent.com/mlresearch/v270/main/assets/koirala25a/koirala25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
